# Test Models Overview

This directory contains four representative deep learning models used for testing and benchmarking system performance:

- **BERT**: Bidirectional Encoder Representations from Transformers
- **GPT**: Generative Pre-trained Transformer
- **Llama** (In preparation)

## Usage in Our Paper

Among these models, **BERT** and **GPT** are selected as the primary benchmarks in our paper to evaluate the effectiveness of the proposed system. They are widely used in natural language processing tasks and serve as strong representatives of modern Transformer-based architectures.

## Purpose

These models are included to:

- Demonstrate the generality of the system across different model structures
- Measure the system's impact on various workloads
- Provide reproducible test cases for further research

Feel free to use or modify these models to suit your experimental setup.
